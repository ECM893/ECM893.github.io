<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ecm893.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ecm893.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-28T04:01:59+00:00</updated><id>https://ecm893.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Neural Net Optimization, Part 1 — The Computational Graph</title><link href="https://ecm893.github.io/blog/2025/ml-levels-pt-one/" rel="alternate" type="text/html" title="Neural Net Optimization, Part 1 — The Computational Graph"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://ecm893.github.io/blog/2025/ml-levels-pt-one</id><content type="html" xml:base="https://ecm893.github.io/blog/2025/ml-levels-pt-one/"><![CDATA[<h1 id="in-the-beginning">In the Beginning</h1> <p>Welcome to the first post in a three-part series on neural network optimization! My goal is to share a practical perspective on how the scale and complexity of neural network optimization evolves, especially for those who, like me, started without much formal training. Over the years, I’ve helped several early-stage companies move from a handful of Jupyter notebooks to more robust, scalable ML workflows. This series chronicles that journey and the lessons learned along the way.</p> <h2 id="three-levels-of-neural-network-implementation">Three Levels of Neural Network Implementation</h2> <ol> <li> <p><strong>Computational Graph:</strong><br/> A simple computational graph network, built from scratch, capable of a forward pass (post-order traversal) and auto-differentiation (backward pass) to fit any defined network to a set of data points.</p> </li> <li> <p><strong>PyTorch Example:</strong><br/> A minimal PyTorch implementation, capable of more complex learning on a basic multilayer perceptron. This demonstrates how frameworks can simplify and scale up what’s possible.</p> </li> <li> <p><strong>Scaling Up:</strong><br/> The real-world challenge: distributing training and tuning across multiple GPUs, or running many experiments in parallel on a single GPU.</p> </li> </ol> <hr/> <h2 id="the-repository">The Repository</h2> <p>All code for this post is available on <a href="https://github.com/ECM893/computational_graph">GitHub</a>.</p> <hr/> <h1 id="the-computational-graph">The Computational Graph</h1> <p>This is the third time in my life I’ve built this toy example from scratch, and I promise myself it’ll be the last! But it’s a fantastic way to understand the basics of neural networks, how they’re optimized, and why frameworks like PyTorch exist.</p> <p><strong>If you’ve ever wondered what’s happening “under the hood” in PyTorch or TensorFlow, this is it: at their core, these frameworks build and manipulate computational graphs just like this one—except they do it automatically, at scale, and with hardware acceleration. Every neural network you define in PyTorch is internally represented as a computational graph, where each operation (like addition, multiplication, or activation functions) becomes a node, and the framework handles the forward and backward passes for you.</strong></p> <p>The computational graph here is made from a few simple nodes (constants, inputs, addition, multiplication, and power). A graph object manages these nodes, letting you interact with the network as a whole rather than node-by-node.</p> <hr/> <h2 id="why-this-is-hard-to-scale">Why This Is Hard to Scale</h2> <p>While building a computational graph from scratch is a great learning exercise, it quickly becomes impractical for real-world ML tasks. Here are two key reasons:</p> <h3 id="1-complexity-and-scalability">1. Complexity and Scalability</h3> <ul> <li><strong>Manual Graph Construction:</strong><br/> Every operation and connection must be explicitly defined. As your network grows, this becomes tedious and error-prone.</li> <li><strong>No GPU Support:</strong><br/> Hand-rolled computational graphs are CPU-bound and not designed for parallel computation. Scaling to larger models or datasets (let alone running on a GPU) is a massive challenge.</li> <li><strong>Debugging:</strong><br/> When things go wrong, it’s hard to tell if the issue is with your math, your code, or your hyperparameters.</li> </ul> <h3 id="2-the-hyperparameter-trap">2. The Hyperparameter Trap</h3> <ul> <li><strong>Tuning Is Everything:</strong><br/> Even if your code is mathematically correct, poor hyperparameter choices (like learning rate or gradient clipping) can make your model look broken—or make a working model look perfect. The difference between “Is there a bug in my code?” and “Wow, this works perfectly!” often comes down to a few numbers.</li> </ul> <hr/> <h2 id="example-fitting-a-polynomial">Example: Fitting a Polynomial</h2> <p>Take the example in my repository, <code class="language-plaintext highlighter-rouge">example_polynomial_fit.py</code>.<br/> It fits a dataset of the form:</p> \[y = c_0 x_1^2 + c_1 x_1 + c_2 x_2^2 + c_3 x_2 + c_4\] <p>using a hand-built computational graph.</p> <hr/> <h2 id="the-good-when-it-works">The Good: When It Works</h2> <p>With reasonable hyperparameters, the model converges smoothly:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MAX_GRAD</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">MIN_LR</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">LR_DECAY</span> <span class="o">=</span> <span class="mf">0.999</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">0.01</span>
</code></pre></div></div> <style>.responsive-iframe-container{position:relative;width:100%;max-width:100%;aspect-ratio:16 / 9;height:auto;overflow:hidden;min-height:300px}.responsive-iframe-container.box{aspect-ratio:1 / 1;min-height:250px}.responsive-iframe-container iframe{position:absolute;inset:0;width:100%;min-width:0;max-width:100%;height:100%;min-height:0;max-height:100%;border:0;display:block;overflow:hidden;background:transparent;box-sizing:border-box}@media(max-width:600px){.responsive-iframe-container{aspect-ratio:unset;min-height:220px;height:220px}.responsive-iframe-container.box{aspect-ratio:unset;min-height:180px;height:180px}}</style> <p>update 7</p> <div class="responsive-iframe-container"> <iframe src="/assets/plotly/losses.html" allowfullscreen="" sandbox="allow-scripts allow-same-origin"></iframe> </div> <div class="responsive-iframe-container box"> <iframe src="/assets/plotly/model_surface.html" allowfullscreen="" sandbox="allow-scripts allow-same-origin"></iframe> </div> <hr/> <h2 id="the-bad-when-it-doesnt">The Bad: When It Doesn’t</h2> <p>But with a bad learning rate (and uncapped gradients), the solution becomes unstable and never converges:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MAX_GRAD</span> <span class="o">=</span> <span class="mf">1e10</span>
<span class="n">MIN_LR</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">LR_DECAY</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">0.1</span>
</code></pre></div></div> <div class="responsive-iframe-container"> <iframe src="/assets/plotly/losses_bad.html" allowfullscreen="" sandbox="allow-scripts allow-same-origin"></iframe> </div> <div class="responsive-iframe-container box"> <iframe src="/assets/plotly/model_surface_bad.html" allowfullscreen="" sandbox="allow-scripts allow-same-origin"></iframe> </div> <hr/> <h2 id="key-takeaways">Key Takeaways</h2> <ul> <li><strong>Building from scratch is educational, but not scalable.</strong><br/> As soon as you want to build even simple perceptron or run on a GPU, you’ll hit a wall.</li> <li><strong>Hyperparameters matter as much as code.</strong><br/> The right settings can make your model look brilliant; the wrong ones can make you question everything.</li> <li><strong>Frameworks exist for a reason.</strong><br/> PyTorch and TensorFlow automate graph construction, GPU support, and much of the tuning process so you can focus on solving real problems.</li> </ul> <hr/> <p>Stay tuned for Part 2, where we’ll use PyTorch to build a more practical neural network and see how much easier (and more powerful) things become!</p>]]></content><author><name>ECM</name></author><category term="blog-posts"/><category term="ML"/><category term="neural-networks"/><category term="optimization"/><category term="computational-graph"/><category term="pytorch"/><category term="hyperparameters"/><summary type="html"><![CDATA[The first in a three-part series exploring neural network optimization, starting from scratch with a toy computational graph and building up to scalable, practical ML systems.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ecm893.github.io/assets/images/neural-net-graph.png"/><media:content medium="image" url="https://ecm893.github.io/assets/images/neural-net-graph.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry></feed>