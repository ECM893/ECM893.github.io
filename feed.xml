<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ecm893.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ecm893.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-23T14:19:07+00:00</updated><id>https://ecm893.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Neural Net Optimization, Part 3 — PyTorch and PyTorch Lightning</title><link href="https://ecm893.github.io/blog/2025/ml-levels-pt-three/" rel="alternate" type="text/html" title="Neural Net Optimization, Part 3 — PyTorch and PyTorch Lightning"/><published>2025-06-20T00:00:00+00:00</published><updated>2025-06-20T00:00:00+00:00</updated><id>https://ecm893.github.io/blog/2025/ml-levels-pt-three</id><content type="html" xml:base="https://ecm893.github.io/blog/2025/ml-levels-pt-three/"><![CDATA[<h1 id="pytorch-and-other-tools">PyTorch and Other Tools</h1> <p>Let’s talk about a problem that I come across frequently: Training loops for any type of neural net architecture can be rather involved and inflexible, yet despite their complexity, they all rely on very similar boilerplate code. Starting out, or even for experienced developers, it can be very frustrating indeed to debug and notice that you’ve dropped a model.train() somewhere in your code, or that you forgot to call model.eval() before running inference.</p> <p>I so wish I had known about <a href="https://www.pytorchlightning.ai/">PyTorch Lightning</a> when starting out.</p> <p>PyTorch Lightning wraps PyTorch models in a class that handles training loops as part of methods of its same class, so that you do not need to explicitly write the loops! All we need to do is give the PyTorch Lightning object our optimization criteria, stoppage criteria, etc.</p> <p>Let’s briefly compare how you might implement a simple neural network in both PyTorch and PyTorch Lightning, and discuss why you might choose one over the other.</p> <h2 id="minimal-pytorch-example">Minimal PyTorch Example</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># Simple model
</span><span class="k">def</span> <span class="nf">make_model</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nf">make_model</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="c1"># Training loop (boilerplate)
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div> <h2 id="pytorch-lightning-example">PyTorch Lightning Example</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pytorch_lightning</span> <span class="k">as</span> <span class="n">pl</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="n">pl</span><span class="p">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Trainer handles loops, logging, etc.
# trainer = pl.Trainer(max_epochs=100)
# trainer.fit(LitModel(), dataloader)
</span></code></pre></div></div> <h3 id="why-use-pytorch-lightning">Why Use PyTorch Lightning?</h3> <ul> <li><strong>Removes boilerplate:</strong> Handles training loops, logging, checkpointing, and more.</li> <li><strong>Scalability:</strong> Makes it easy to scale to multiple GPUs or TPUs.</li> <li><strong>Reproducibility:</strong> Standardizes code structure, making experiments easier to reproduce.</li> </ul> <h3 id="why-use-pytorch-not-pytorch-lightning">Why Use PyTorch (NOT PyTorch Lightning)?</h3> <ul> <li><strong>Full control needed:</strong> If you need to customize every detail of the training loop or use highly experimental features, raw PyTorch may be preferable.</li> <li><strong>Very simple scripts:</strong> For quick experiments or teaching, plain PyTorch can be more transparent.</li> </ul> <hr/> <h2 id="hyperparameter-optimization-the-next-challenge">Hyperparameter Optimization: The Next Challenge</h2> <p>PyTorch Lightning helps reduce code clutter, but it doesn’t solve the problem of choosing the right hyperparameters. In fact, as models become more complex, the number of hyperparameters grows rapidly:</p> <ul> <li><strong>Learning rate, batch size, optimizer type</strong></li> <li><strong>Number of layers, layer sizes, activation functions</strong></li> <li><strong>Dropout rates, weight decay, initialization schemes</strong></li> <li><strong>Data augmentation parameters</strong></li> <li><strong>Scheduler settings, early stopping criteria</strong></li> <li><strong>Architecture choices (e.g., skip connections, normalization layers)</strong></li> </ul> <p>Even the number of layers, their types, and their sizes become hyperparameters! The search space can be enormous, and brute-force search is rarely practical.</p> <p>This leads to the next major challenge: <strong>hyperparameter optimization</strong>. Even with all the boilerplate removed, finding the best configuration is still a hard problem—just as we saw with simpler models in Part 1.</p> <hr/> <p><em>In Part 4, we’ll explore strategies for navigating this vast hyperparameter space using a package/api called <code class="language-plaintext highlighter-rouge">ray</code>, from grid search to implement more advanced optimization techniques.</em></p>]]></content><author><name>ECM</name></author><category term="blog-posts"/><category term="ML"/><category term="neural-networks"/><category term="optimization"/><category term="pytorch"/><category term="pytorch-lightning"/><category term="hyperparameters"/><summary type="html"><![CDATA[The third in a four-part series on neural network optimization, focusing on PyTorch fundamentals, abstraction with PyTorch Lightning, and the ongoing challenge of hyperparameter tuning.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ecm893.github.io/assets/images/pytorch-vs-lightning.png"/><media:content medium="image" url="https://ecm893.github.io/assets/images/pytorch-vs-lightning.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Neural Net Optimization, Part 2 — PyTorch Writ Small</title><link href="https://ecm893.github.io/blog/2025/ml-levels-pt-two/" rel="alternate" type="text/html" title="Neural Net Optimization, Part 2 — PyTorch Writ Small"/><published>2025-06-06T00:00:00+00:00</published><updated>2025-06-06T00:00:00+00:00</updated><id>https://ecm893.github.io/blog/2025/ml-levels-pt-two</id><content type="html" xml:base="https://ecm893.github.io/blog/2025/ml-levels-pt-two/"><![CDATA[<h1 id="fine-grained-pytorch-what-does-a-neuron-really-learn">Fine-Grained PyTorch: What Does a Neuron Really Learn?</h1> <p>In <a href="./2025-05-23-ml-levels-pt-one">Part 1</a>, we fit a third-order polynomial equation to build a computational graph. Here in Part 2, we’ll see what a single PyTorch neuron can do.</p> <hr/> <h2 id="the-single-pytorch-linear-neuron">The Single PyTorch Linear Neuron</h2> <p>A single linear neuron in PyTorch fits functions of the form:</p> \[y = w x + b\] <p>This function produces a first-order polynomial – just a straight line. No matter how complex our data, a single linear neuron can only learn a straight line:</p> <p><img src="/assets/img/ml-levels-pt-two/combined_regression.png" alt="Single Neuron Regression Fits"/></p> <p><em>Figure: Fitting a sine wave with a single linear neuron.</em></p> <hr/> <h2 id="adding-nonlinearity-relu">Adding Nonlinearity: ReLU</h2> <p>Adding a ReLU activation gives us a “kink,” but still only allows for piecewise linear fits:</p> <pre><code class="language-math">f(x) = \max(0, x)
</code></pre> <p><img src="/assets/img/ml-levels-pt-two/combined_regression_relu.png" alt="Single Neuron Regression Fits"/></p> <p><em>Figure: Fitting a sine wave with a single ReLU neuron.</em></p> <hr/> <h2 id="more-neurons-more-segments">More Neurons, More Segments</h2> <p>What if we used two neurons in a hidden layer with ReLU activations? Now we can fit a piecewise linear function with two segments:</p> <div> <a href="#" onclick="var e=document.getElementById('collapse-2n'); e.style.display = (e.style.display==='none') ? 'block' : 'none'; return false;"> <strong>Show 2 neurons regression fit</strong> </a> <div id="collapse-2n" style="display:none; margin-top:10px;"> <img src="/assets/img/ml-levels-pt-two/combined_regression_2n.png" alt="Multiple Neurons Regression Fits"/> </div> </div> <p>By adding more neurons, the network can fit more piecewise linear segments, but it’s not able to accommodate smooth curves, like a sine wave. The network is fundamentally limited by the linearity of each segment.</p> <p>It is common to include functions that have curves, e.g. using a tanH activation function instead of ReLU. However, these are typically only implemented in the final output layer.</p> <hr/> <h2 id="key-takeaways">Key Takeaways</h2> <ul> <li><strong>Single linear neuron:</strong> fits a straight line.</li> <li><strong>Single ReLU neuron:</strong> fits a line with a kink.</li> <li><strong>Multiple ReLU neurons:</strong> fits a piecewise linear function.</li> <li><strong>Generalization:</strong> Even with more neurons, the network struggles to extrapolate or capture smooth nonlinearities outside the training region.</li> </ul> <p>PyTorch builds complexity by stacking simple layers, not by fitting high-order polynomials directly. Understanding these basics helps explain both the power and the limitations of neural networks.</p> <hr/> <h2 id="discussion">Discussion</h2> <p>Neural networks are powerful because they combine many simple units. However, each unit has inherent limitations. Understanding these limits is crucial for building intuition about what your models can and cannot learn, and why extrapolation is often unreliable.</p> <p>This highlights some fundamental concepts that are often overlooked in the rush to build complex models:</p> <ol> <li> <p><strong>Piecewise Linear Nature:</strong><br/> Neural networks are fundamentally based on piecewise approximations of small linear segments. The more neurons and layers you add, the more complex the piecewise function becomes. However, at each layer, the transformations are still linear in nature. This limitation is important to keep in mind when interpreting the capabilities of your model.</p> </li> <li> <p><strong>Training Range Limitations:</strong><br/> Neural networks can only reliably predict values within the range of the data they were trained on. Extrapolation beyond the training data is inherently unreliable. This raises an important question:<br/> <em>What is the range of the data when dealing with images, high-dimensional data, or language data?</em><br/> Defining the range of such data in a meaningful way is a challenging and often philosophical problem.</p> </li> </ol> <p>While state-of-the-art neural networks can achieve remarkable feats—such as passing a Turing test, it’s important to remember that they are <strong>approximations of the underlying functions they are trying to model</strong>. This has significant implications for how we interpret their outputs and understand their limitations in generalization.</p> <p>(But they are REALLY good approximations!)</p> <hr/> <blockquote> <p><strong>Thought Experiment:</strong><br/> <em>“If you give an AI enough information about an elephant, does it implicitly know about a cat?”</em><br/> — Someone, somewhere on the internet (probably)</p> </blockquote> <p>No.</p> <p>This simple thought experiment underscores the importance of understanding the limitations of neural networks. They do not “know” or “understand” concepts outside the scope of their training data, they approximate patterns within the data they’ve seen.</p> <hr/> <p><em>In <a href="./2025-06-20-ml-levels-pt-three">Part 3</a>, we’ll explore how to scale up these concepts with PyTorch Lightning, focusing on abstraction and the ongoing challenge of hyperparameter tuning.</em></p>]]></content><author><name>ECM</name></author><category term="blog-posts"/><category term="ML"/><category term="neural-networks"/><category term="optimization"/><category term="pytorch"/><category term="pytorch-lightning"/><category term="hyperparameters"/><summary type="html"><![CDATA[The second in a four-part series on neural network optimization, focusing on PyTorch fundamentals and how to implement PyTorch on a per neuron level.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ecm893.github.io/assets/images/pytorch-vs-lightning.png"/><media:content medium="image" url="https://ecm893.github.io/assets/images/pytorch-vs-lightning.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Neural Net Optimization, Part 1 — The Computational Graph</title><link href="https://ecm893.github.io/blog/2025/ml-levels-pt-one/" rel="alternate" type="text/html" title="Neural Net Optimization, Part 1 — The Computational Graph"/><published>2025-05-23T00:00:00+00:00</published><updated>2025-05-23T00:00:00+00:00</updated><id>https://ecm893.github.io/blog/2025/ml-levels-pt-one</id><content type="html" xml:base="https://ecm893.github.io/blog/2025/ml-levels-pt-one/"><![CDATA[<h1 id="in-the-beginning">In the Beginning</h1> <p>Welcome to the first post in a three-part series on neural network optimization! My goal is to share a practical perspective on how the scale and complexity of neural network optimization evolves, especially for those who, like me, started without much formal training. Over the years, I’ve helped several early-stage companies move from a handful of Jupyter notebooks to more robust, scalable ML workflows. This series chronicles that journey and the lessons learned along the way.</p> <h2 id="three-levels-of-neural-network-implementation">Three Levels of Neural Network Implementation</h2> <ol> <li> <p><strong>Computational Graph:</strong><br/> A simple computational graph network, built from scratch, capable of a forward pass (post-order traversal) and auto-differentiation (backward pass) to fit any defined network to a set of data points.</p> </li> <li> <p><strong>PyTorch Example:</strong><br/> A minimal PyTorch implementation, capable of more complex learning on a basic multilayer perceptron. This demonstrates how frameworks can simplify and scale up what’s possible.</p> </li> <li> <p><strong>Scaling Up:</strong><br/> The real-world challenge: distributing training and tuning across multiple GPUs, or running many experiments in parallel on a single GPU.</p> </li> </ol> <hr/> <h2 id="the-repository">The Repository</h2> <p>All code for this post is available on <a href="https://github.com/ECM893/computational_graph">GitHub</a>.</p> <hr/> <h1 id="the-computational-graph">The Computational Graph</h1> <p>This is the third time in my life I’ve built this toy example from scratch, and I promise myself it’ll be the last! But it’s a fantastic way to understand the basics of neural networks, how they’re optimized, and why frameworks like PyTorch exist.</p> <p><strong>If you’ve ever wondered what’s happening “under the hood” in PyTorch or TensorFlow, this is it: at their core, these frameworks build and manipulate computational graphs just like this one—except they do it automatically, at scale, and with hardware acceleration. Every neural network you define in PyTorch is internally represented as a computational graph, where each operation (like addition, multiplication, or activation functions) becomes a node, and the framework handles the forward and backward passes for you.</strong></p> <p>The computational graph here is made from a few simple nodes (constants, inputs, addition, multiplication, and power). A graph object manages these nodes, letting you interact with the network as a whole rather than node-by-node.</p> <hr/> <h2 id="why-this-is-hard-to-scale">Why This Is Hard to Scale</h2> <p>While building a computational graph from scratch is a great learning exercise, it quickly becomes impractical for real-world ML tasks. Here are two key reasons:</p> <h3 id="1-complexity-and-scalability">1. Complexity and Scalability</h3> <ul> <li><strong>Manual Graph Construction:</strong><br/> Every operation and connection must be explicitly defined. As your network grows, this becomes tedious and error-prone.</li> <li><strong>No GPU Support:</strong><br/> Hand-rolled computational graphs are CPU-bound and not designed for parallel computation. Scaling to larger models or datasets (let alone running on a GPU) is a massive challenge.</li> <li><strong>Debugging:</strong><br/> When things go wrong, it’s hard to tell if the issue is with your math, your code, or your hyperparameters.</li> </ul> <h3 id="2-the-hyperparameter-trap">2. The Hyperparameter Trap</h3> <ul> <li><strong>Tuning Is Everything:</strong><br/> Even if your code is mathematically correct, poor hyperparameter choices (like learning rate or gradient clipping) can make your model look broken—or make a working model look perfect. The difference between “Is there a bug in my code?” and “Wow, this works perfectly!” often comes down to a few numbers.</li> </ul> <hr/> <h2 id="example-fitting-a-polynomial">Example: Fitting a Polynomial</h2> <p>Take the example in my repository, <code class="language-plaintext highlighter-rouge">example_polynomial_fit.py</code>.<br/> It fits a dataset of the form:</p> \[y = c_0 x_1^2 + c_1 x_1 + c_2 x_2^2 + c_3 x_2 + c_4\] <p>using a hand-built computational graph.</p> <hr/> <h2 id="the-good-when-it-works">The Good: When It Works</h2> <p>With reasonable hyperparameters, the model converges smoothly:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MAX_GRAD</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">MIN_LR</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">LR_DECAY</span> <span class="o">=</span> <span class="mf">0.999</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">0.01</span>
</code></pre></div></div> <style>.responsive-iframe-container{position:relative;width:100%;max-width:100%;aspect-ratio:16 / 9;height:auto;overflow:hidden;min-height:300px}.responsive-iframe-container.box{aspect-ratio:1 / 1;min-height:250px}.responsive-iframe-container iframe{position:absolute;inset:0;width:100%;min-width:0;max-width:100%;height:100%;min-height:0;max-height:100%;border:0;display:block;overflow:hidden;background:transparent;box-sizing:border-box}@media(max-width:600px){.responsive-iframe-container{aspect-ratio:unset;min-height:220px;height:220px}.responsive-iframe-container.box{aspect-ratio:unset;min-height:180px;height:180px}}</style> <p>update 7</p> <div class="responsive-iframe-container"> <iframe src="/assets/plotly/losses.html" allowfullscreen="" sandbox="allow-scripts allow-same-origin"></iframe> </div> <div class="responsive-iframe-container box"> <iframe src="/assets/plotly/model_surface.html" allowfullscreen="" sandbox="allow-scripts allow-same-origin"></iframe> </div> <hr/> <h2 id="the-bad-when-it-doesnt">The Bad: When It Doesn’t</h2> <p>But with a bad learning rate (and uncapped gradients), the solution becomes unstable and never converges:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MAX_GRAD</span> <span class="o">=</span> <span class="mf">1e10</span>
<span class="n">MIN_LR</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">LR_DECAY</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">0.1</span>
</code></pre></div></div> <div class="responsive-iframe-container"> <iframe src="/assets/plotly/losses_bad.html" allowfullscreen="" sandbox="allow-scripts allow-same-origin"></iframe> </div> <div class="responsive-iframe-container box"> <iframe src="/assets/plotly/model_surface_bad.html" allowfullscreen="" sandbox="allow-scripts allow-same-origin"></iframe> </div> <hr/> <h2 id="key-takeaways">Key Takeaways</h2> <ul> <li><strong>Building from scratch is educational, but not scalable.</strong><br/> As soon as you want to build even simple perceptron or run on a GPU, you’ll hit a wall.</li> <li><strong>Hyperparameters matter as much as code.</strong><br/> The right settings can make your model look brilliant; the wrong ones can make you question everything.</li> <li><strong>Frameworks exist for a reason.</strong><br/> PyTorch and TensorFlow automate graph construction, GPU support, and much of the tuning process so you can focus on solving real problems.</li> </ul> <hr/> <p><em>Stay tuned for Part 2, where we’ll use PyTorch to build a more practical neural network and see how much easier (and more powerful) things become!</em></p>]]></content><author><name>ECM</name></author><category term="blog-posts"/><category term="ML"/><category term="neural-networks"/><category term="optimization"/><category term="computational-graph"/><category term="pytorch"/><category term="hyperparameters"/><summary type="html"><![CDATA[The first in a four-part series exploring neural network optimization, starting from scratch with a toy computational graph and building up to scalable, practical ML systems.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ecm893.github.io/assets/images/neural-net-graph.png"/><media:content medium="image" url="https://ecm893.github.io/assets/images/neural-net-graph.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry></feed>